{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ReyhanehRazavi-99/AlexNet-vs-VisionTransformer/blob/main/AlexNet_on_CIFAR10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDDVt12zn7dq",
        "outputId": "77360043-d937-4cec-ce79-a4a4cd4ff7ca"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:13<00:00, 12.5MB/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 233M/233M [00:01<00:00, 242MB/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting train features...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting features: 100%|██████████| 391/391 [00:47<00:00,  8.18it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting test features...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting features: 100%|██████████| 79/79 [00:09<00:00,  8.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train feats: (50000, 9216) Test feats: (10000, 9216)\n",
            "Training Linear SVM on frozen AlexNet features...\n",
            "\n",
            "Test Accuracy: 71.95%\n",
            "\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    airplane       0.74      0.73      0.74      1000\n",
            "  automobile       0.81      0.81      0.81      1000\n",
            "        bird       0.64      0.63      0.64      1000\n",
            "         cat       0.55      0.55      0.55      1000\n",
            "        deer       0.68      0.67      0.68      1000\n",
            "         dog       0.65      0.65      0.65      1000\n",
            "        frog       0.76      0.76      0.76      1000\n",
            "       horse       0.74      0.73      0.74      1000\n",
            "        ship       0.80      0.84      0.82      1000\n",
            "       truck       0.81      0.82      0.81      1000\n",
            "\n",
            "    accuracy                           0.72     10000\n",
            "   macro avg       0.72      0.72      0.72     10000\n",
            "weighted avg       0.72      0.72      0.72     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#type 2\n",
        "\n",
        "# ============================\n",
        "# CIFAR-10 + AlexNet (no fine-tuning)\n",
        "# Feature extraction + Linear SVM classifier\n",
        "# ============================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# ----------------------------\n",
        "# Reproducibility (optional)\n",
        "# ----------------------------\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# ----------------------------\n",
        "# Device\n",
        "# ----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ----------------------------\n",
        "# Data: CIFAR-10\n",
        "# AlexNet expects 224x224 and ImageNet normalization\n",
        "# ----------------------------\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
        "\n",
        "transform_eval = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "])\n",
        "\n",
        "train_ds = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_eval)\n",
        "test_ds  = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_eval)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# ----------------------------\n",
        "# Model: AlexNet (pretrained on ImageNet), used as a frozen feature extractor\n",
        "# We do NOT modify weights, and we do NOT fine-tune.\n",
        "# ----------------------------\n",
        "weights = models.AlexNet_Weights.IMAGENET1K_V1\n",
        "alexnet = models.alexnet(weights=weights)\n",
        "alexnet.eval()  # inference mode\n",
        "alexnet.to(device)\n",
        "\n",
        "# We’ll grab the feature vector *before* the classifier:\n",
        "# feature pipeline is: features -> avgpool -> flatten -> classifier\n",
        "# The flattened feature dimension is 256*6*6 = 9216 for AlexNet with 224x224 input.\n",
        "@torch.no_grad()\n",
        "def extract_features(dataloader):\n",
        "    feats_list = []\n",
        "    labels_list = []\n",
        "    for images, labels in tqdm(dataloader, desc=\"Extracting features\"):\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        # forward through convolutional backbone\n",
        "        x = alexnet.features(images)\n",
        "        x = alexnet.avgpool(x)\n",
        "        x = torch.flatten(x, 1)  # shape: [B, 9216]\n",
        "        feats_list.append(x.cpu().numpy())\n",
        "        labels_list.append(labels.numpy())\n",
        "    feats = np.concatenate(feats_list, axis=0)\n",
        "    labs  = np.concatenate(labels_list, axis=0)\n",
        "    return feats, labs\n",
        "\n",
        "print(\"Extracting train features...\")\n",
        "X_train, y_train = extract_features(train_loader)\n",
        "print(\"Extracting test features...\")\n",
        "X_test,  y_test  = extract_features(test_loader)\n",
        "\n",
        "print(\"Train feats:\", X_train.shape, \"Test feats:\", X_test.shape)\n",
        "\n",
        "# ----------------------------\n",
        "# Classifier: Linear SVM (no deep learning training here)\n",
        "# Standardize features -> LinearSVC\n",
        "# ----------------------------\n",
        "clf = Pipeline([\n",
        "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "    (\"svm\", LinearSVC(C=1.0, max_iter=10000, dual=True))\n",
        "])\n",
        "\n",
        "print(\"Training Linear SVM on frozen AlexNet features...\")\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# ----------------------------\n",
        "# Evaluate\n",
        "# ----------------------------\n",
        "y_pred = clf.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nTest Accuracy: {acc*100:.2f}%\\n\")\n",
        "print(\"Classification report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=train_ds.classes))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# CIFAR-10 + AlexNet (Type 2 = Partial Fine-Tune)\n",
        "# Unfreeze the last block (tail of conv backbone) + full classifier; keep earlier backbone frozen\n",
        "# ============================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# ----------------------------\n",
        "# Reproducibility (optional)\n",
        "# ----------------------------\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# ----------------------------\n",
        "# Device\n",
        "# ----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ----------------------------\n",
        "# Data: CIFAR-10\n",
        "# Resize to 224 and use ImageNet normalization\n",
        "# ----------------------------\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "])\n",
        "\n",
        "transform_eval = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "])\n",
        "\n",
        "train_ds = datasets.CIFAR10(root=\"./data\", train=True,  download=True, transform=transform_train)\n",
        "test_ds  = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_eval)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True,  num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "num_classes = 10\n",
        "class_names = train_ds.classes\n",
        "\n",
        "# ----------------------------\n",
        "# Model: AlexNet pretrained; PARTIAL fine-tune\n",
        "# - Freeze early conv layers\n",
        "# - Unfreeze the LAST conv \"block\" + the ENTIRE classifier\n",
        "# ----------------------------\n",
        "weights = models.AlexNet_Weights.IMAGENET1K_V1\n",
        "model = models.alexnet(weights=weights)\n",
        "\n",
        "# Replace final layer to match CIFAR-10\n",
        "in_features = model.classifier[6].in_features  # 4096\n",
        "model.classifier[6] = nn.Linear(in_features, num_classes)\n",
        "\n",
        "# 1) Freeze EVERYTHING first\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# 2) Unfreeze the last conv block (tail of model.features)\n",
        "#    AlexNet features structure indices (PyTorch torchvision):\n",
        "#    0:Conv,1:ReLU,2:Pool,3:Conv,4:ReLU,5:Pool,\n",
        "#    6:Conv,7:ReLU,8:Conv,9:ReLU,10:Conv,11:ReLU,12:Pool\n",
        "#    We'll unfreeze indices >= 10 (last Conv + ReLU + final Pool)\n",
        "last_block_start = 10\n",
        "for idx, layer in enumerate(model.features):\n",
        "    if idx >= last_block_start:\n",
        "        for p in layer.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "# 3) Unfreeze the ENTIRE classifier head\n",
        "for p in model.classifier.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# ----------------------------\n",
        "# Loss / Optimizer / Scheduler\n",
        "# Use two LR groups: smaller for last conv block, larger for classifier\n",
        "# ----------------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Collect params for two groups\n",
        "conv_tail_params = []\n",
        "for idx, layer in enumerate(model.features):\n",
        "    if idx >= last_block_start:\n",
        "        conv_tail_params += list(layer.parameters())\n",
        "\n",
        "classifier_params = list(model.classifier.parameters())\n",
        "\n",
        "optimizer = optim.AdamW([\n",
        "    {\"params\": conv_tail_params,    \"lr\": 3e-5, \"weight_decay\": 1e-4},  # conservative on conv tail\n",
        "    {\"params\": classifier_params,   \"lr\": 1e-3, \"weight_decay\": 1e-4},  # bigger LR for classifier\n",
        "], betas=(0.9, 0.999))\n",
        "\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-5)\n",
        "\n",
        "# ----------------------------\n",
        "# Train / Eval helpers\n",
        "# ----------------------------\n",
        "def train_one_epoch(model, loader):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    for images, labels in tqdm(loader, desc=\"Train\", leave=False):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits = model(images)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        preds = logits.argmax(1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total   += labels.size(0)\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, desc=\"Eval\"):\n",
        "    model.eval()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    all_preds, all_labels = [], []\n",
        "    for images, labels in tqdm(loader, desc=desc, leave=False):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        logits = model(images)\n",
        "        loss = criterion(logits, labels)\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        preds = logits.argmax(1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total   += labels.size(0)\n",
        "        all_preds.append(preds.cpu().numpy())\n",
        "        all_labels.append(labels.cpu().numpy())\n",
        "    avg_loss = running_loss / total\n",
        "    acc = correct / total\n",
        "    y_pred = np.concatenate(all_preds)\n",
        "    y_true = np.concatenate(all_labels)\n",
        "    return avg_loss, acc, y_true, y_pred\n",
        "\n",
        "# ----------------------------\n",
        "# Train (partial fine-tune)\n",
        "# ----------------------------\n",
        "epochs = 10\n",
        "for ep in range(1, epochs + 1):\n",
        "    tr_loss, tr_acc = train_one_epoch(model, train_loader)\n",
        "    te_loss, te_acc, _, _ = evaluate(model, test_loader, desc=\"Test\")\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch {ep:02d} | Train Loss {tr_loss:.4f} Acc {tr_acc*100:.2f}% | \"\n",
        "          f\"Test Loss {te_loss:.4f} Acc {te_acc*100:.2f}%\")\n",
        "\n",
        "# ----------------------------\n",
        "# Final evaluation with report\n",
        "# ----------------------------\n",
        "_, test_acc, y_true, y_pred = evaluate(model, test_loader, desc=\"Final Test\")\n",
        "print(f\"\\nFinal Test Accuracy (Partial Fine-Tune): {test_acc*100:.2f}%\\n\")\n",
        "print(\"Classification report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phNf1yORF0yp",
        "outputId": "f9457a47-c2d8-457a-d2ed-5657be57ba95"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:06<00:00, 24.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 233M/233M [00:00<00:00, 249MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Train Loss 0.7492 Acc 74.20% | Test Loss 0.5458 Acc 80.80%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 02 | Train Loss 0.5659 Acc 80.48% | Test Loss 0.4926 Acc 82.93%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 03 | Train Loss 0.4872 Acc 83.36% | Test Loss 0.4813 Acc 83.26%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 04 | Train Loss 0.4269 Acc 85.34% | Test Loss 0.4314 Acc 84.87%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 05 | Train Loss 0.3731 Acc 87.08% | Test Loss 0.4040 Acc 86.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 06 | Train Loss 0.3309 Acc 88.46% | Test Loss 0.3966 Acc 86.38%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 07 | Train Loss 0.2836 Acc 90.14% | Test Loss 0.3770 Acc 87.22%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 08 | Train Loss 0.2463 Acc 91.41% | Test Loss 0.3616 Acc 87.92%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 09 | Train Loss 0.2219 Acc 92.29% | Test Loss 0.3561 Acc 88.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 | Train Loss 0.2018 Acc 92.92% | Test Loss 0.3557 Acc 88.23%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                           "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Test Accuracy (Partial Fine-Tune): 88.23%\n",
            "\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    airplane     0.9069    0.8860    0.8963      1000\n",
            "  automobile     0.9301    0.9320    0.9311      1000\n",
            "        bird     0.9057    0.8360    0.8695      1000\n",
            "         cat     0.7812    0.7710    0.7760      1000\n",
            "        deer     0.8449    0.8880    0.8659      1000\n",
            "         dog     0.8535    0.8040    0.8280      1000\n",
            "        frog     0.9096    0.9160    0.9128      1000\n",
            "       horse     0.8999    0.9080    0.9039      1000\n",
            "        ship     0.9091    0.9400    0.9243      1000\n",
            "       truck     0.8820    0.9420    0.9110      1000\n",
            "\n",
            "    accuracy                         0.8823     10000\n",
            "   macro avg     0.8823    0.8823    0.8819     10000\n",
            "weighted avg     0.8823    0.8823    0.8819     10000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jXobQ3r7F0wS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P0wqwnjUF0rW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IzwrHSOJF0nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#type 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# ----------------------------\n",
        "# Reproducibility (optional)\n",
        "# ----------------------------\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# ----------------------------\n",
        "# Device\n",
        "# ----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ----------------------------\n",
        "# Data: CIFAR-10\n",
        "# AlexNet expects 224x224 and ImageNet normalization\n",
        "# ----------------------------\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
        "\n",
        "# Keep transforms similar to your original (no augmentation for a clean probe)\n",
        "transform_eval = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "])\n",
        "\n",
        "train_ds = datasets.CIFAR10(root=\"./data\", train=True,  download=True, transform=transform_eval)\n",
        "test_ds  = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_eval)\n",
        "\n",
        "# For training the linear head, it's better to shuffle the training set\n",
        "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True,  num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# ----------------------------\n",
        "# Model: AlexNet pretrained on ImageNet\n",
        "# - Freeze all parameters (backbone + earlier classifier layers)\n",
        "# - Replace final classifier layer with new Linear to 10 classes\n",
        "# ----------------------------\n",
        "weights = models.AlexNet_Weights.IMAGENET1K_V1\n",
        "model = models.alexnet(weights=weights)\n",
        "\n",
        "# Replace final layer (classifier[6]) — input dim is 4096 for AlexNet\n",
        "in_features = model.classifier[6].in_features\n",
        "model.classifier[6] = nn.Linear(in_features, 10)\n",
        "\n",
        "# Freeze EVERYTHING first\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Unfreeze ONLY the final layer\n",
        "for p in model.classifier[6].parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "model = model.to(device)\n",
        "print(\"Trainable params (should be only final layer):\",\n",
        "      sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "\n",
        "# ----------------------------\n",
        "# Loss / Optimizer\n",
        "# ----------------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.classifier[6].parameters(), lr=1e-3, weight_decay=0.0)\n",
        "\n",
        "# Optional: a simple cosine schedule over a small number of epochs\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-5)\n",
        "\n",
        "# ----------------------------\n",
        "# Train / Eval helpers\n",
        "# ----------------------------\n",
        "@torch.no_grad()\n",
        "def evaluate(loader, model):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    for images, labels in loader:\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "        logits = model(images)\n",
        "        preds = logits.argmax(1)\n",
        "        all_preds.append(preds.cpu().numpy())\n",
        "        all_labels.append(labels.cpu().numpy())\n",
        "    y_pred = np.concatenate(all_preds)\n",
        "    y_true = np.concatenate(all_labels)\n",
        "    acc = (y_pred == y_true).mean()\n",
        "    return acc, y_true, y_pred\n",
        "\n",
        "def train_linear_probe(epochs=10, print_every=100):\n",
        "    model.train()\n",
        "    n_batches = len(train_loader)\n",
        "    for ep in range(1, epochs + 1):\n",
        "        running_loss, correct, total = 0.0, 0, 0\n",
        "        for b, (images, labels) in enumerate(train_loader, 1):\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            logits = model(images)                 # full forward, but only final layer has grads\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            preds = logits.argmax(1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total   += labels.size(0)\n",
        "\n",
        "            if b % print_every == 0 or b == n_batches:\n",
        "                print(f\"Epoch {ep}/{epochs} | Batch {b}/{n_batches} | \"\n",
        "                      f\"Loss {running_loss/b:.4f} | Acc {(correct/total)*100:.2f}%\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "# ----------------------------\n",
        "# Train the linear head\n",
        "# ----------------------------\n",
        "train_linear_probe(epochs=10, print_every=100)\n",
        "\n",
        "# ----------------------------\n",
        "# Evaluate on test set\n",
        "# ----------------------------\n",
        "test_acc, y_true, y_pred = evaluate(test_loader, model)\n",
        "print(f\"\\nTest Accuracy (linear probe): {test_acc*100:.2f}%\\n\")\n",
        "print(\"Classification report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=train_ds.classes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pP4JfXeTY-zD",
        "outputId": "405463fa-723a-444d-cee5-5c6230382551"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Trainable params (should be only final layer): 40970\n",
            "Epoch 1/10 | Batch 100/391 | Loss 0.9062 | Acc 68.46%\n",
            "Epoch 1/10 | Batch 200/391 | Loss 0.8198 | Acc 71.16%\n",
            "Epoch 1/10 | Batch 300/391 | Loss 0.7742 | Acc 72.79%\n",
            "Epoch 1/10 | Batch 391/391 | Loss 0.7497 | Acc 73.73%\n",
            "Epoch 2/10 | Batch 100/391 | Loss 0.6272 | Acc 77.68%\n",
            "Epoch 2/10 | Batch 200/391 | Loss 0.6411 | Acc 77.47%\n",
            "Epoch 2/10 | Batch 300/391 | Loss 0.6359 | Acc 77.51%\n",
            "Epoch 2/10 | Batch 391/391 | Loss 0.6331 | Acc 77.63%\n",
            "Epoch 3/10 | Batch 100/391 | Loss 0.5845 | Acc 79.16%\n",
            "Epoch 3/10 | Batch 200/391 | Loss 0.6064 | Acc 78.60%\n",
            "Epoch 3/10 | Batch 300/391 | Loss 0.6007 | Acc 78.85%\n",
            "Epoch 3/10 | Batch 391/391 | Loss 0.5997 | Acc 78.83%\n",
            "Epoch 4/10 | Batch 100/391 | Loss 0.5712 | Acc 79.93%\n",
            "Epoch 4/10 | Batch 200/391 | Loss 0.5717 | Acc 80.00%\n",
            "Epoch 4/10 | Batch 300/391 | Loss 0.5749 | Acc 79.79%\n",
            "Epoch 4/10 | Batch 391/391 | Loss 0.5802 | Acc 79.59%\n",
            "Epoch 5/10 | Batch 100/391 | Loss 0.5524 | Acc 80.15%\n",
            "Epoch 5/10 | Batch 200/391 | Loss 0.5459 | Acc 80.52%\n",
            "Epoch 5/10 | Batch 300/391 | Loss 0.5551 | Acc 80.26%\n",
            "Epoch 5/10 | Batch 391/391 | Loss 0.5578 | Acc 80.14%\n",
            "Epoch 6/10 | Batch 100/391 | Loss 0.5447 | Acc 80.89%\n",
            "Epoch 6/10 | Batch 200/391 | Loss 0.5346 | Acc 81.19%\n",
            "Epoch 6/10 | Batch 300/391 | Loss 0.5345 | Acc 81.12%\n",
            "Epoch 6/10 | Batch 391/391 | Loss 0.5383 | Acc 80.93%\n",
            "Epoch 7/10 | Batch 100/391 | Loss 0.5258 | Acc 81.11%\n",
            "Epoch 7/10 | Batch 200/391 | Loss 0.5267 | Acc 81.27%\n",
            "Epoch 7/10 | Batch 300/391 | Loss 0.5236 | Acc 81.27%\n",
            "Epoch 7/10 | Batch 391/391 | Loss 0.5258 | Acc 81.19%\n",
            "Epoch 8/10 | Batch 100/391 | Loss 0.5128 | Acc 81.37%\n",
            "Epoch 8/10 | Batch 200/391 | Loss 0.5179 | Acc 81.46%\n",
            "Epoch 8/10 | Batch 300/391 | Loss 0.5155 | Acc 81.64%\n",
            "Epoch 8/10 | Batch 391/391 | Loss 0.5144 | Acc 81.75%\n",
            "Epoch 9/10 | Batch 100/391 | Loss 0.4989 | Acc 82.37%\n",
            "Epoch 9/10 | Batch 200/391 | Loss 0.5003 | Acc 82.35%\n",
            "Epoch 9/10 | Batch 300/391 | Loss 0.5038 | Acc 82.16%\n",
            "Epoch 9/10 | Batch 391/391 | Loss 0.5035 | Acc 82.09%\n",
            "Epoch 10/10 | Batch 100/391 | Loss 0.5014 | Acc 82.34%\n",
            "Epoch 10/10 | Batch 200/391 | Loss 0.5016 | Acc 82.19%\n",
            "Epoch 10/10 | Batch 300/391 | Loss 0.4985 | Acc 82.24%\n",
            "Epoch 10/10 | Batch 391/391 | Loss 0.4953 | Acc 82.33%\n",
            "\n",
            "Test Accuracy (linear probe): 83.61%\n",
            "\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    airplane       0.85      0.83      0.84      1000\n",
            "  automobile       0.89      0.90      0.89      1000\n",
            "        bird       0.86      0.73      0.79      1000\n",
            "         cat       0.73      0.71      0.72      1000\n",
            "        deer       0.79      0.82      0.80      1000\n",
            "         dog       0.82      0.80      0.81      1000\n",
            "        frog       0.81      0.91      0.86      1000\n",
            "       horse       0.85      0.85      0.85      1000\n",
            "        ship       0.89      0.91      0.90      1000\n",
            "       truck       0.88      0.90      0.89      1000\n",
            "\n",
            "    accuracy                           0.84     10000\n",
            "   macro avg       0.84      0.84      0.84     10000\n",
            "weighted avg       0.84      0.84      0.84     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5SBZCp9j4lGk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818
        },
        "outputId": "0f22e912-6d1d-4c5a-b4e6-a9c32a66ac86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [01:26<00:00, 1.97MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 233M/233M [00:03<00:00, 75.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Train Loss 0.7742 Acc 72.87% | Test Loss 0.5787 Acc 79.43%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 02 | Train Loss 0.6624 Acc 76.75% | Test Loss 0.5660 Acc 80.01%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 03 | Train Loss 0.6293 Acc 77.86% | Test Loss 0.5508 Acc 80.60%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 04 | Train Loss 0.6136 Acc 78.50% | Test Loss 0.5308 Acc 81.10%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 05 | Train Loss 0.5925 Acc 78.89% | Test Loss 0.5213 Acc 81.54%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 06 | Train Loss 0.5777 Acc 79.50% | Test Loss 0.5288 Acc 81.63%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 07 | Train Loss 0.5635 Acc 80.13% | Test Loss 0.5384 Acc 81.10%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 08 | Train Loss 0.5507 Acc 80.54% | Test Loss 0.5011 Acc 82.43%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 09 | Train Loss 0.5483 Acc 80.54% | Test Loss 0.4972 Acc 82.68%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 | Train Loss 0.5366 Acc 81.02% | Test Loss 0.4961 Acc 82.76%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Test Accuracy (Linear Probe): 82.76%\n",
            "\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    airplane     0.8542    0.7970    0.8246      1000\n",
            "  automobile     0.8811    0.8890    0.8850      1000\n",
            "        bird     0.8770    0.7060    0.7823      1000\n",
            "         cat     0.7297    0.6910    0.7098      1000\n",
            "        deer     0.7680    0.8210    0.7936      1000\n",
            "         dog     0.7982    0.7910    0.7946      1000\n",
            "        frog     0.8451    0.8840    0.8641      1000\n",
            "       horse     0.8368    0.8510    0.8438      1000\n",
            "        ship     0.8505    0.9160    0.8820      1000\n",
            "       truck     0.8409    0.9300    0.8832      1000\n",
            "\n",
            "    accuracy                         0.8276     10000\n",
            "   macro avg     0.8281    0.8276    0.8263     10000\n",
            "weighted avg     0.8281    0.8276    0.8263     10000\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1607469735.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;31m# use a fixed size (5k) so class balance is nice; adjust as you like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m X_tr, X_val, y_tr, y_val = train_test_split(\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m )\n\u001b[1;32m    171\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train feats:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"| Val feats:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"| Test feats:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ],
      "source": [
        "# ============================\n",
        "# CIFAR-10 + AlexNet (Linear Probe)\n",
        "# Freeze all backbone weights; train final layer only\n",
        "# ============================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# ----------------------------\n",
        "# Reproducibility (optional)\n",
        "# ----------------------------\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# ----------------------------\n",
        "# Device\n",
        "# ----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ----------------------------\n",
        "# Data: CIFAR-10\n",
        "# Resize to 224 and use ImageNet normalization\n",
        "# ----------------------------\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "])\n",
        "\n",
        "transform_eval = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "])\n",
        "\n",
        "train_ds = datasets.CIFAR10(root=\"./data\", train=True,  download=True, transform=transform_train)\n",
        "test_ds  = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_eval)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True,  num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "num_classes = 10\n",
        "class_names = train_ds.classes\n",
        "\n",
        "# ----------------------------\n",
        "# Model: AlexNet pretrained; freeze backbone; replace final layer\n",
        "# ----------------------------\n",
        "weights = models.AlexNet_Weights.IMAGENET1K_V1\n",
        "model = models.alexnet(weights=weights)\n",
        "\n",
        "# Freeze ALL parameters first (backbone + classifier)\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Replace ONLY the last linear layer to match CIFAR-10 and train it\n",
        "in_features = model.classifier[6].in_features  # 4096\n",
        "model.classifier[6] = nn.Linear(in_features, num_classes)\n",
        "\n",
        "# Make sure the new head is trainable\n",
        "for p in model.classifier[6].parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# ----------------------------\n",
        "# Loss / Optimizer / Scheduler\n",
        "# ----------------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.classifier[6].parameters(), lr=1e-3, weight_decay=0.0)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-5)\n",
        "\n",
        "# ----------------------------\n",
        "# Train / Eval helpers\n",
        "# ----------------------------\n",
        "def train_one_epoch(model, loader):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    for images, labels in tqdm(loader, desc=\"Train\", leave=False):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits = model(images)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        preds = logits.argmax(1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total   += labels.size(0)\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, desc=\"Eval\"):\n",
        "    model.eval()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    all_preds, all_labels = [], []\n",
        "    for images, labels in tqdm(loader, desc=desc, leave=False):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        logits = model(images)\n",
        "        loss = criterion(logits, labels)\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        preds = logits.argmax(1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total   += labels.size(0)\n",
        "        all_preds.append(preds.cpu().numpy())\n",
        "        all_labels.append(labels.cpu().numpy())\n",
        "    avg_loss = running_loss / total\n",
        "    acc = correct / total\n",
        "    y_pred = np.concatenate(all_preds)\n",
        "    y_true = np.concatenate(all_labels)\n",
        "    return avg_loss, acc, y_true, y_pred\n",
        "\n",
        "# ----------------------------\n",
        "# Train (linear probe)\n",
        "# ----------------------------\n",
        "epochs = 10\n",
        "for ep in range(1, epochs + 1):\n",
        "    tr_loss, tr_acc = train_one_epoch(model, train_loader)\n",
        "    te_loss, te_acc, _, _ = evaluate(model, test_loader, desc=\"Test\")\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch {ep:02d} | Train Loss {tr_loss:.4f} Acc {tr_acc*100:.2f}% | \"\n",
        "          f\"Test Loss {te_loss:.4f} Acc {te_acc*100:.2f}%\")\n",
        "\n",
        "# ----------------------------\n",
        "# Final evaluation with report\n",
        "# ----------------------------\n",
        "_, test_acc, y_true, y_pred = evaluate(model, test_loader, desc=\"Final Test\")\n",
        "print(f\"\\nFinal Test Accuracy (Linear Probe): {test_acc*100:.2f}%\\n\")\n",
        "print(\"Classification report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
        " # ============================\n",
        "# Try multiple classifiers on frozen AlexNet features\n",
        "# ============================\n",
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# ---- split a validation set out of the extracted train features\n",
        "# use a fixed size (5k) so class balance is nice; adjust as you like\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=5000, stratify=y_train, random_state=42\n",
        ")\n",
        "print(\"Train feats:\", X_tr.shape, \"| Val feats:\", X_val.shape, \"| Test feats:\", X_test.shape)\n",
        "\n",
        "def fit_eval(name, clf, Xtr, ytr, Xva, yva, Xte, yte):\n",
        "    clf.fit(Xtr, ytr)\n",
        "    y_val_pred = clf.predict(Xva); val_acc = accuracy_score(yva, y_val_pred)\n",
        "    y_te_pred  = clf.predict(Xte); te_acc  = accuracy_score(yte, y_te_pred)\n",
        "    print(f\"{name:>20} | Val: {val_acc*100:6.2f}% | Test: {te_acc*100:6.2f}%\")\n",
        "    return {\"name\": name, \"model\": clf, \"val_acc\": val_acc, \"test_acc\": te_acc, \"y_test_pred\": y_te_pred}\n",
        "\n",
        "print(\"\\n=== Comparing classifiers on frozen AlexNet features (9216-d) ===\")\n",
        "models = OrderedDict()\n",
        "\n",
        "# 1) Linear SVM (strong linear baseline)\n",
        "models[\"LinearSVM\"] = Pipeline([\n",
        "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "    (\"clf\", LinearSVC(C=1.0, max_iter=20000, dual=True))\n",
        "])\n",
        "\n",
        "# 2) Multinomial Logistic Regression (often close to LinearSVM)\n",
        "models[\"LogReg_LBFGS\"] = Pipeline([\n",
        "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "    (\"clf\", LogisticRegression(C=1.0, max_iter=3000, n_jobs=-1, multi_class=\"multinomial\", solver=\"lbfgs\"))\n",
        "])\n",
        "\n",
        "# 3) RBF SVM — PCA→256 for speed/stability\n",
        "models[\"SVM_RBF_PCA256\"] = Pipeline([\n",
        "    (\"pca\", PCA(n_components=256, random_state=42)),\n",
        "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "    (\"clf\", SVC(kernel=\"rbf\", C=5.0, gamma=\"scale\"))\n",
        "])\n",
        "\n",
        "# 4) Linear SGD (hinge) — very fast linear baseline\n",
        "models[\"SGD_Hinge\"] = Pipeline([\n",
        "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "    (\"clf\", SGDClassifier(loss=\"hinge\", alpha=1e-4, max_iter=3000, tol=1e-3, n_jobs=-1, random_state=42))\n",
        "])\n",
        "\n",
        "# 5) kNN — PCA→128 then scale (distance-friendly)\n",
        "models[\"kNN_PCA128_k7\"] = Pipeline([\n",
        "    (\"pca\", PCA(n_components=128, random_state=42)),\n",
        "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "    (\"clf\", KNeighborsClassifier(n_neighbors=7, n_jobs=-1))\n",
        "])\n",
        "\n",
        "# 6) Random Forest — tree-based (no scaling)\n",
        "models[\"RandomForest400\"] = Pipeline([\n",
        "    (\"clf\", RandomForestClassifier(n_estimators=400, max_depth=None, n_jobs=-1, random_state=42))\n",
        "])\n",
        "\n",
        "# 7) MLP — shallow NN on features\n",
        "models[\"MLP_512\"] = Pipeline([\n",
        "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "    (\"clf\", MLPClassifier(hidden_layer_sizes=(512,), activation=\"relu\",\n",
        "                          alpha=1e-4, batch_size=256, learning_rate_init=1e-3,\n",
        "                          max_iter=200, random_state=42))\n",
        "])\n",
        "\n",
        "# ---- evaluate all\n",
        "results = []\n",
        "for name, pipe in models.items():\n",
        "    res = fit_eval(name, pipe, X_tr, y_tr, X_val, y_val, X_test, y_test)\n",
        "    results.append(res)\n",
        "\n",
        "# ---- pick best by validation accuracy\n",
        "results_sorted = sorted(results, key=lambda r: r[\"val_acc\"], reverse=True)\n",
        "best = results_sorted[0]\n",
        "print(\"\\n=== Summary (sorted by Val acc) ===\")\n",
        "for r in results_sorted:\n",
        "    print(f\"{r['name']:>20} | Val: {r['val_acc']*100:6.2f}% | Test: {r['test_acc']*100:6.2f}%\")\n",
        "print(f\"\\nBest on validation: {best['name']}\")\n",
        "\n",
        "# ---- refit best on FULL train (train + val), then final test report\n",
        "X_trva = np.concatenate([X_tr, X_val], axis=0)\n",
        "y_trva = np.concatenate([y_tr, y_val], axis=0)\n",
        "\n",
        "best_model = models[best[\"name\"]]\n",
        "best_model.fit(X_trva, y_trva)\n",
        "y_test_pred_final = best_model.predict(X_test)\n",
        "final_test_acc = accuracy_score(y_test, y_test_pred_final)\n",
        "\n",
        "print(f\"\\n=== FINAL (refit on train+val) — {best['name']} ===\")\n",
        "print(f\"Final Test Accuracy: {final_test_acc*100:.2f}%\")\n",
        "print(\"\\nClassification report (Test):\")\n",
        "print(classification_report(y_test, y_test_pred_final, target_names=train_ds.classes, digits=4))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_test_pred_final)\n",
        "print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
        "print(cm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dn7XsCAF--p",
        "outputId": "b9117762-566b-4c56-b684-05aa9ddaeb26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:10<00:00, 15.7MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 45000 | Val: 5000 | Test: 10000\n",
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 233M/233M [00:01<00:00, 123MB/s]\n",
            "/tmp/ipython-input-1621946242.py:112: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
            "Epoch 1/40:   0%|                                                           | 0/352 [00:00<?, ?it/s]/tmp/ipython-input-1621946242.py:150: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
            "Epoch 1/40: 100%|████████████| 352/352 [00:58<00:00,  6.01it/s, train_loss=0.7254, train_acc=74.81%]\n",
            "/tmp/ipython-input-1621946242.py:127: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] val_loss=0.4850 | val_acc=82.58% | lr=0.00998\n",
            "✓ Saved new best model to checkpoints/alexnet_cifar10_best.pt (val_acc=82.58%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/40: 100%|████████████| 352/352 [00:42<00:00,  8.29it/s, train_loss=0.4131, train_acc=85.88%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 2] val_loss=0.3807 | val_acc=86.66% | lr=0.00994\n",
            "✓ Saved new best model to checkpoints/alexnet_cifar10_best.pt (val_acc=86.66%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/40: 100%|████████████| 352/352 [00:42<00:00,  8.33it/s, train_loss=0.3157, train_acc=89.06%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 3] val_loss=0.3462 | val_acc=87.82% | lr=0.00986\n",
            "✓ Saved new best model to checkpoints/alexnet_cifar10_best.pt (val_acc=87.82%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/40: 100%|████████████| 352/352 [00:46<00:00,  7.51it/s, train_loss=0.2456, train_acc=91.39%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 4] val_loss=0.3732 | val_acc=87.18% | lr=0.00976\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/40: 100%|████████████| 352/352 [00:47<00:00,  7.47it/s, train_loss=0.1929, train_acc=93.25%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 5] val_loss=0.3233 | val_acc=88.26% | lr=0.00962\n",
            "✓ Saved new best model to checkpoints/alexnet_cifar10_best.pt (val_acc=88.26%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/40: 100%|████████████| 352/352 [00:42<00:00,  8.30it/s, train_loss=0.1512, train_acc=94.63%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 6] val_loss=0.3133 | val_acc=89.40% | lr=0.00946\n",
            "✓ Saved new best model to checkpoints/alexnet_cifar10_best.pt (val_acc=89.40%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/40: 100%|████████████| 352/352 [00:42<00:00,  8.32it/s, train_loss=0.1226, train_acc=95.66%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 7] val_loss=0.3323 | val_acc=89.20% | lr=0.00926\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/40: 100%|████████████| 352/352 [00:42<00:00,  8.32it/s, train_loss=0.1030, train_acc=96.37%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 8] val_loss=0.3538 | val_acc=89.16% | lr=0.00905\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/40: 100%|████████████| 352/352 [00:43<00:00,  8.18it/s, train_loss=0.0815, train_acc=97.14%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 9] val_loss=0.3253 | val_acc=89.94% | lr=0.00880\n",
            "✓ Saved new best model to checkpoints/alexnet_cifar10_best.pt (val_acc=89.94%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/40: 100%|███████████| 352/352 [00:47<00:00,  7.48it/s, train_loss=0.0694, train_acc=97.62%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 10] val_loss=0.3529 | val_acc=89.96% | lr=0.00854\n",
            "✓ Saved new best model to checkpoints/alexnet_cifar10_best.pt (val_acc=89.96%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/40: 100%|███████████| 352/352 [00:42<00:00,  8.37it/s, train_loss=0.0605, train_acc=97.92%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 11] val_loss=0.3548 | val_acc=89.62% | lr=0.00825\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/40: 100%|███████████| 352/352 [00:42<00:00,  8.30it/s, train_loss=0.0517, train_acc=98.22%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 12] val_loss=0.3346 | val_acc=90.38% | lr=0.00794\n",
            "✓ Saved new best model to checkpoints/alexnet_cifar10_best.pt (val_acc=90.38%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/40: 100%|███████████| 352/352 [00:42<00:00,  8.34it/s, train_loss=0.0386, train_acc=98.70%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 13] val_loss=0.3488 | val_acc=90.52% | lr=0.00761\n",
            "✓ Saved new best model to checkpoints/alexnet_cifar10_best.pt (val_acc=90.52%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/40: 100%|███████████| 352/352 [00:42<00:00,  8.26it/s, train_loss=0.0349, train_acc=98.81%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 14] val_loss=0.3374 | val_acc=90.64% | lr=0.00727\n",
            "✓ Saved new best model to checkpoints/alexnet_cifar10_best.pt (val_acc=90.64%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/40: 100%|███████████| 352/352 [00:42<00:00,  8.33it/s, train_loss=0.0289, train_acc=99.06%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 15] val_loss=0.3096 | val_acc=91.02% | lr=0.00691\n",
            "✓ Saved new best model to checkpoints/alexnet_cifar10_best.pt (val_acc=91.02%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/40: 100%|███████████| 352/352 [00:42<00:00,  8.25it/s, train_loss=0.0257, train_acc=99.13%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 16] val_loss=0.3504 | val_acc=90.50% | lr=0.00655\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17/40: 100%|███████████| 352/352 [00:42<00:00,  8.35it/s, train_loss=0.0208, train_acc=99.37%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 17] val_loss=0.3434 | val_acc=90.94% | lr=0.00617\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18/40: 100%|███████████| 352/352 [00:46<00:00,  7.53it/s, train_loss=0.0178, train_acc=99.45%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 18] val_loss=0.3438 | val_acc=91.30% | lr=0.00578\n",
            "✓ Saved new best model to checkpoints/alexnet_cifar10_best.pt (val_acc=91.30%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19/40: 100%|███████████| 352/352 [00:41<00:00,  8.38it/s, train_loss=0.0134, train_acc=99.63%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 19] val_loss=0.3319 | val_acc=91.30% | lr=0.00539\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20/40: 100%|███████████| 352/352 [00:41<00:00,  8.41it/s, train_loss=0.0106, train_acc=99.71%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 20] val_loss=0.3337 | val_acc=91.42% | lr=0.00500\n",
            "✓ Saved new best model to checkpoints/alexnet_cifar10_best.pt (val_acc=91.42%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 21/40: 100%|███████████| 352/352 [00:45<00:00,  7.74it/s, train_loss=0.0090, train_acc=99.78%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 21] val_loss=0.3275 | val_acc=91.62% | lr=0.00461\n",
            "✓ Saved new best model to checkpoints/alexnet_cifar10_best.pt (val_acc=91.62%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 22/40: 100%|███████████| 352/352 [00:41<00:00,  8.42it/s, train_loss=0.0080, train_acc=99.80%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 22] val_loss=0.3132 | val_acc=91.78% | lr=0.00422\n",
            "✓ Saved new best model to checkpoints/alexnet_cifar10_best.pt (val_acc=91.78%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 23/40: 100%|███████████| 352/352 [00:41<00:00,  8.40it/s, train_loss=0.0070, train_acc=99.84%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 23] val_loss=0.3279 | val_acc=92.06% | lr=0.00383\n",
            "✓ Saved new best model to checkpoints/alexnet_cifar10_best.pt (val_acc=92.06%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 24/40: 100%|███████████| 352/352 [00:41<00:00,  8.42it/s, train_loss=0.0061, train_acc=99.85%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 24] val_loss=0.3437 | val_acc=91.74% | lr=0.00345\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 25/40: 100%|███████████| 352/352 [00:41<00:00,  8.39it/s, train_loss=0.0045, train_acc=99.89%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 25] val_loss=0.3298 | val_acc=91.82% | lr=0.00309\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 26/40: 100%|███████████| 352/352 [00:41<00:00,  8.43it/s, train_loss=0.0045, train_acc=99.90%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 26] val_loss=0.3237 | val_acc=92.12% | lr=0.00273\n",
            "✓ Saved new best model to checkpoints/alexnet_cifar10_best.pt (val_acc=92.12%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 27/40: 100%|███████████| 352/352 [00:41<00:00,  8.44it/s, train_loss=0.0037, train_acc=99.92%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 27] val_loss=0.3379 | val_acc=91.74% | lr=0.00239\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 28/40: 100%|███████████| 352/352 [00:41<00:00,  8.43it/s, train_loss=0.0034, train_acc=99.94%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 28] val_loss=0.3301 | val_acc=92.02% | lr=0.00206\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 29/40: 100%|███████████| 352/352 [00:41<00:00,  8.45it/s, train_loss=0.0028, train_acc=99.96%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 29] val_loss=0.3293 | val_acc=92.22% | lr=0.00175\n",
            "✓ Saved new best model to checkpoints/alexnet_cifar10_best.pt (val_acc=92.22%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 30/40: 100%|███████████| 352/352 [00:41<00:00,  8.42it/s, train_loss=0.0027, train_acc=99.95%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 30] val_loss=0.3201 | val_acc=92.10% | lr=0.00146\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 31/40: 100%|███████████| 352/352 [00:41<00:00,  8.45it/s, train_loss=0.0029, train_acc=99.95%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 31] val_loss=0.3202 | val_acc=92.14% | lr=0.00120\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 32/40: 100%|███████████| 352/352 [00:41<00:00,  8.39it/s, train_loss=0.0026, train_acc=99.96%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 32] val_loss=0.3207 | val_acc=92.26% | lr=0.00095\n",
            "✓ Saved new best model to checkpoints/alexnet_cifar10_best.pt (val_acc=92.26%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 33/40: 100%|███████████| 352/352 [00:41<00:00,  8.39it/s, train_loss=0.0027, train_acc=99.96%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 33] val_loss=0.3194 | val_acc=92.28% | lr=0.00074\n",
            "✓ Saved new best model to checkpoints/alexnet_cifar10_best.pt (val_acc=92.28%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 34/40: 100%|███████████| 352/352 [00:41<00:00,  8.39it/s, train_loss=0.0021, train_acc=99.97%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 34] val_loss=0.3233 | val_acc=92.22% | lr=0.00054\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 35/40: 100%|███████████| 352/352 [00:41<00:00,  8.47it/s, train_loss=0.0021, train_acc=99.98%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 35] val_loss=0.3228 | val_acc=92.36% | lr=0.00038\n",
            "✓ Saved new best model to checkpoints/alexnet_cifar10_best.pt (val_acc=92.36%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 36/40: 100%|███████████| 352/352 [00:41<00:00,  8.43it/s, train_loss=0.0019, train_acc=99.99%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 36] val_loss=0.3242 | val_acc=92.22% | lr=0.00024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 37/40: 100%|███████████| 352/352 [00:41<00:00,  8.50it/s, train_loss=0.0020, train_acc=99.97%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 37] val_loss=0.3240 | val_acc=92.26% | lr=0.00014\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 38/40: 100%|███████████| 352/352 [00:42<00:00,  8.36it/s, train_loss=0.0024, train_acc=99.96%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 38] val_loss=0.3226 | val_acc=92.28% | lr=0.00006\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 39/40: 100%|███████████| 352/352 [00:41<00:00,  8.45it/s, train_loss=0.0021, train_acc=99.98%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 39] val_loss=0.3224 | val_acc=92.24% | lr=0.00002\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 40/40: 100%|███████████| 352/352 [00:43<00:00,  8.07it/s, train_loss=0.0020, train_acc=99.98%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 40] val_loss=0.3225 | val_acc=92.24% | lr=0.00000\n",
            "\n",
            "Best epoch: 35  |  Test loss: 0.3660  |  Test acc: 91.78%\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# CIFAR-10 — Full Fine-Tuning of AlexNet (PyTorch)\n",
        "# - Uses ImageNet-pretrained AlexNet, replaces head with 10 classes\n",
        "# - Trains ALL layers (no freezing)\n",
        "# - Data augmentation, mixed precision, cosine LR, best checkpoint\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# ----------------------------\n",
        "# Reproducibility\n",
        "# ----------------------------\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.benchmark = True  # speedup for fixed image size\n",
        "\n",
        "# ----------------------------\n",
        "# Device\n",
        "# ----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ----------------------------\n",
        "# Hyperparameters\n",
        "# ----------------------------\n",
        "BATCH_SIZE   = 128\n",
        "EPOCHS       = 40\n",
        "BASE_LR      = 0.01\n",
        "WEIGHT_DECAY = 5e-4\n",
        "MOMENTUM     = 0.9\n",
        "VAL_SPLIT    = 5000   # from the 50k training images\n",
        "NUM_WORKERS  = 2\n",
        "SAVE_DIR     = Path(\"./checkpoints\")\n",
        "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "BEST_PATH    = SAVE_DIR / \"alexnet_cifar10_best.pt\"\n",
        "\n",
        "# ----------------------------\n",
        "# Transforms (AlexNet expects 224x224 + ImageNet norm)\n",
        "# ----------------------------\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.6, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "])\n",
        "\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "])\n",
        "\n",
        "# ----------------------------\n",
        "# Datasets & Loaders\n",
        "# ----------------------------\n",
        "root = \"./data\"\n",
        "full_train = datasets.CIFAR10(root=root, train=True, download=True, transform=train_transform)\n",
        "test_set   = datasets.CIFAR10(root=root, train=False, download=True, transform=eval_transform)\n",
        "\n",
        "# Split train -> train/val (keep class distribution approx. random)\n",
        "train_size = len(full_train) - VAL_SPLIT\n",
        "val_size   = VAL_SPLIT\n",
        "train_set, val_set = random_split(full_train, [train_size, val_size], generator=torch.Generator().manual_seed(seed))\n",
        "\n",
        "# Important: use eval_transform for val\n",
        "val_set.dataset.transform = eval_transform\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=True)\n",
        "val_loader   = DataLoader(val_set,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "test_loader  = DataLoader(test_set,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "print(f\"Train: {len(train_set)} | Val: {len(val_set)} | Test: {len(test_set)}\")\n",
        "\n",
        "# ----------------------------\n",
        "# Model: AlexNet (pretrained) -> head to 10 classes\n",
        "# ----------------------------\n",
        "weights = models.AlexNet_Weights.IMAGENET1K_V1\n",
        "model = models.alexnet(weights=weights)\n",
        "\n",
        "# Replace classifier last layer (4096 -> 10)\n",
        "in_feats = model.classifier[6].in_features\n",
        "model.classifier[6] = nn.Linear(in_feats, 10)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# ----------------------------\n",
        "# Loss, Optimizer, Scheduler\n",
        "# ----------------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=BASE_LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY, nesterov=True)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "# ----------------------------\n",
        "# Utilities\n",
        "# ----------------------------\n",
        "def accuracy(logits, targets):\n",
        "    preds = logits.argmax(dim=1)\n",
        "    return (preds == targets).float().mean().item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(loader, desc=\"Eval\"):\n",
        "    model.eval()\n",
        "    total_loss, total_acc, total = 0.0, 0.0, 0\n",
        "    for imgs, labels in loader:\n",
        "        imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, labels)\n",
        "        bs = labels.size(0)\n",
        "        total += bs\n",
        "        total_loss += loss.item() * bs\n",
        "        total_acc  += accuracy(logits, labels) * bs\n",
        "    return total_loss / total, total_acc / total\n",
        "\n",
        "# ----------------------------\n",
        "# Train Loop\n",
        "# ----------------------------\n",
        "best_val_acc = 0.0\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    running_loss, running_acc, seen = 0.0, 0.0, 0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", ncols=100)\n",
        "    for imgs, labels in pbar:\n",
        "        imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        bs = labels.size(0)\n",
        "        seen += bs\n",
        "        running_loss += loss.item() * bs\n",
        "        running_acc  += accuracy(logits, labels) * bs\n",
        "\n",
        "        pbar.set_postfix({\n",
        "            \"train_loss\": f\"{running_loss/seen:.4f}\",\n",
        "            \"train_acc\":  f\"{100*running_acc/seen:.2f}%\"\n",
        "        })\n",
        "\n",
        "    # Validation\n",
        "    val_loss, val_acc = evaluate(val_loader, desc=\"Val\")\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"[Epoch {epoch}] val_loss={val_loss:.4f} | val_acc={100*val_acc:.2f}% | lr={scheduler.get_last_lr()[0]:.5f}\")\n",
        "\n",
        "    # Save best\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"state_dict\": model.state_dict(),\n",
        "            \"val_acc\": val_acc,\n",
        "        }, BEST_PATH)\n",
        "        print(f\"✓ Saved new best model to {BEST_PATH} (val_acc={100*val_acc:.2f}%)\")\n",
        "\n",
        "# ----------------------------\n",
        "# Test Evaluation (Best Checkpoint)\n",
        "# ----------------------------\n",
        "ckpt = torch.load(BEST_PATH, map_location=device)\n",
        "model.load_state_dict(ckpt[\"state_dict\"])\n",
        "test_loss, test_acc = evaluate(test_loader, desc=\"Test\")\n",
        "print(f\"\\nBest epoch: {ckpt['epoch']}  |  Test loss: {test_loss:.4f}  |  Test acc: {100*test_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_wAqofHb_jG",
        "outputId": "b06ca189-7246-4347-aed3-e40232767cc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting features for SVM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extract SVM feats:   0%|                                                    | 0/352 [00:00<?, ?it/s]/tmp/ipython-input-2388451934.py:25: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
            "Extract SVM feats: 100%|██████████████████████████████████████████| 352/352 [00:46<00:00,  7.59it/s]\n",
            "Extract SVM feats: 100%|████████████████████████████████████████████| 40/40 [00:05<00:00,  7.49it/s]\n",
            "Extract SVM feats: 100%|████████████████████████████████████████████| 79/79 [00:10<00:00,  7.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature shapes: (45000, 4096) (5000, 4096) (10000, 4096)\n",
            "Training Linear SVM on penultimate features...\n",
            "\n",
            "Validation Accuracy (SVM on penultimate features): 91.88%\n",
            "\n",
            "Test Accuracy (SVM on penultimate features): 91.42%\n",
            "\n",
            "Classification report (Test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    airplane     0.9093    0.9220    0.9156      1000\n",
            "  automobile     0.9504    0.9590    0.9547      1000\n",
            "        bird     0.9005    0.8870    0.8937      1000\n",
            "         cat     0.8407    0.8130    0.8266      1000\n",
            "        deer     0.9043    0.9260    0.9150      1000\n",
            "         dog     0.8742    0.8620    0.8681      1000\n",
            "        frog     0.9376    0.9460    0.9418      1000\n",
            "       horse     0.9422    0.9460    0.9441      1000\n",
            "        ship     0.9413    0.9460    0.9436      1000\n",
            "       truck     0.9378    0.9350    0.9364      1000\n",
            "\n",
            "    accuracy                         0.9142     10000\n",
            "   macro avg     0.9138    0.9142    0.9140     10000\n",
            "weighted avg     0.9138    0.9142    0.9140     10000\n",
            "\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[922   4  14   8   4   0   3   4  28  13]\n",
            " [  6 959   0   2   0   0   0   0   6  27]\n",
            " [ 23   1 887  18  27  17  16   5   3   3]\n",
            " [  9   3  23 813  22  83  25  10   5   7]\n",
            " [  6   0  11  16 926  10  13  16   2   0]\n",
            " [  9   0  22  70  16 862   2  16   0   3]\n",
            " [  2   1  12  21   9   4 946   2   2   1]\n",
            " [  7   0   9  10  18   9   0 946   0   1]\n",
            " [ 21   7   5   7   2   1   2   2 946   7]\n",
            " [  9  34   2   2   0   0   2   3  13 935]]\n"
          ]
        }
      ],
      "source": [
        "# ============================\n",
        "# SVM on AlexNet penultimate features\n",
        "# (append after loading BEST checkpoint and test eval)\n",
        "# ============================\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import itertools\n",
        "\n",
        "# Use eval transforms (no aug) for stable feature extraction\n",
        "train_set.dataset.transform = eval_transform  # already done for val_set above, test_set uses eval_transform by construction\n",
        "\n",
        "svm_train_loader = DataLoader(train_set, batch_size=128, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "svm_val_loader   = DataLoader(val_set,   batch_size=128, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "svm_test_loader  = DataLoader(test_set,  batch_size=128, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "@torch.no_grad()\n",
        "def extract_penultimate_features(dataloader):\n",
        "    model.eval()\n",
        "    X, y = [], []\n",
        "    for imgs, labels in tqdm(dataloader, desc=\"Extract SVM feats\", ncols=100):\n",
        "        imgs = imgs.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "            # forward to penultimate: features -> avgpool -> flatten -> classifier[0..5]\n",
        "            x = model.features(imgs)\n",
        "            x = model.avgpool(x)\n",
        "            x = torch.flatten(x, 1)          # 9216\n",
        "            for i in range(6):                # Dropout, FC(9216->4096), ReLU, Dropout, FC(4096->4096), ReLU\n",
        "                x = model.classifier[i](x)\n",
        "            # x is 4096-dim penultimate features\n",
        "        X.append(x.cpu().numpy())\n",
        "        y.append(labels.cpu().numpy())\n",
        "    return np.concatenate(X, axis=0), np.concatenate(y, axis=0)\n",
        "\n",
        "print(\"\\nExtracting features for SVM...\")\n",
        "X_tr, y_tr = extract_penultimate_features(svm_train_loader)\n",
        "X_va, y_va = extract_penultimate_features(svm_val_loader)\n",
        "X_te, y_te = extract_penultimate_features(svm_test_loader)\n",
        "\n",
        "print(\"Feature shapes:\", X_tr.shape, X_va.shape, X_te.shape)\n",
        "\n",
        "# Linear SVM pipeline (standard and strong baseline)\n",
        "svm_clf = Pipeline([\n",
        "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "    (\"svm\",    LinearSVC(C=1.0, max_iter=20000, dual=True))\n",
        "])\n",
        "\n",
        "print(\"Training Linear SVM on penultimate features...\")\n",
        "svm_clf.fit(X_tr, y_tr)\n",
        "\n",
        "# Evaluate\n",
        "def eval_split(X, y, split_name):\n",
        "    y_pred = svm_clf.predict(X)\n",
        "    acc = accuracy_score(y, y_pred)\n",
        "    print(f\"\\n{split_name} Accuracy (SVM on penultimate features): {acc*100:.2f}%\")\n",
        "    return y_pred, acc\n",
        "\n",
        "y_pred_val, val_acc = eval_split(X_va, y_va, \"Validation\")\n",
        "y_pred_test, test_acc = eval_split(X_te, y_te, \"Test\")\n",
        "\n",
        "# Detailed report (on Test)\n",
        "classes = test_set.classes\n",
        "print(\"\\nClassification report (Test):\")\n",
        "print(classification_report(y_te, y_pred_test, target_names=classes, digits=4))\n",
        "\n",
        "# Confusion matrix (Test)\n",
        "cm = confusion_matrix(y_te, y_pred_test)\n",
        "print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
        "print(cm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4Os_p_lffG-",
        "outputId": "c9d10d66-e5a9-4e58-d3fc-5a699ad5fa20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Comparing classic classifiers on penultimate features ===\n",
            "         LinearSVM | Val:  91.88% | Test:  91.42%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      LogReg_LBFGS | Val:  91.88% | Test:  91.54%\n",
            "    SVM_RBF_PCA256 | Val:  92.50% | Test:  91.85%\n",
            "         SGD_Hinge | Val:  91.40% | Test:  91.15%\n",
            "     kNN_PCA128_k7 | Val:  91.04% | Test:  90.66%\n",
            "   RandomForest400 | Val:  91.58% | Test:  90.79%\n",
            "           MLP_512 | Val:  91.12% | Test:  90.51%\n",
            "\n",
            "=== Summary (sorted by Val acc) ===\n",
            "    SVM_RBF_PCA256 | Val:  92.50% | Test:  91.85%\n",
            "         LinearSVM | Val:  91.88% | Test:  91.42%\n",
            "      LogReg_LBFGS | Val:  91.88% | Test:  91.54%\n",
            "   RandomForest400 | Val:  91.58% | Test:  90.79%\n",
            "         SGD_Hinge | Val:  91.40% | Test:  91.15%\n",
            "           MLP_512 | Val:  91.12% | Test:  90.51%\n",
            "     kNN_PCA128_k7 | Val:  91.04% | Test:  90.66%\n",
            "\n",
            "=== FINAL (refit on train+val) — SVM_RBF_PCA256 ===\n",
            "Final Test Accuracy: 92.01%\n",
            "\n",
            "Classification report (Test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    airplane     0.9136    0.9300    0.9217      1000\n",
            "  automobile     0.9579    0.9550    0.9564      1000\n",
            "        bird     0.9159    0.8930    0.9043      1000\n",
            "         cat     0.8412    0.8370    0.8391      1000\n",
            "        deer     0.9038    0.9300    0.9167      1000\n",
            "         dog     0.8891    0.8740    0.8815      1000\n",
            "        frog     0.9464    0.9540    0.9502      1000\n",
            "       horse     0.9482    0.9340    0.9411      1000\n",
            "        ship     0.9445    0.9530    0.9487      1000\n",
            "       truck     0.9401    0.9410    0.9405      1000\n",
            "\n",
            "    accuracy                         0.9201     10000\n",
            "   macro avg     0.9201    0.9201    0.9200     10000\n",
            "weighted avg     0.9201    0.9201    0.9200     10000\n",
            "\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[930   2  15   7   3   1   2   3  27  10]\n",
            " [  4 955   1   1   0   0   0   0   6  33]\n",
            " [ 22   0 893  22  25  11  14   8   5   0]\n",
            " [  8   2  15 837  20  73  23  10   7   5]\n",
            " [  5   0  12  17 930  11   9  15   1   0]\n",
            " [  5   0  16  72  19 874   3  10   0   1]\n",
            " [  4   0   9  17   9   2 954   3   1   1]\n",
            " [  8   0   8  14  22  11   1 934   0   2]\n",
            " [ 22   6   4   5   1   0   1   0 953   8]\n",
            " [ 10  32   2   3   0   0   1   2   9 941]]\n"
          ]
        }
      ],
      "source": [
        "# ============================\n",
        "# Compare multiple classifiers on penultimate features\n",
        "# (paste after your SVM block; uses X_tr, y_tr, X_va, y_va, X_te, y_te, classes)\n",
        "# ============================\n",
        "from collections import OrderedDict\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "def fit_eval(name, clf, Xtr, ytr, Xva, yva, Xte, yte):\n",
        "    clf.fit(Xtr, ytr)\n",
        "    y_val_pred = clf.predict(Xva)\n",
        "    y_te_pred  = clf.predict(Xte)\n",
        "    val_acc = accuracy_score(yva, y_val_pred)\n",
        "    te_acc  = accuracy_score(yte, y_te_pred)\n",
        "    print(f\"{name:>18} | Val: {val_acc*100:6.2f}% | Test: {te_acc*100:6.2f}%\")\n",
        "    return {\n",
        "        \"name\": name, \"model\": clf,\n",
        "        \"val_acc\": val_acc, \"test_acc\": te_acc,\n",
        "        \"y_val_pred\": y_val_pred, \"y_test_pred\": y_te_pred\n",
        "    }\n",
        "\n",
        "print(\"\\n=== Comparing classic classifiers on penultimate features ===\")\n",
        "models = OrderedDict()\n",
        "\n",
        "# 1) Linear SVM (baseline)\n",
        "models[\"LinearSVM\"] = Pipeline([\n",
        "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "    (\"clf\", LinearSVC(C=1.0, max_iter=20000, dual=True))\n",
        "])\n",
        "\n",
        "# 2) Multinomial Logistic Regression\n",
        "models[\"LogReg_LBFGS\"] = Pipeline([\n",
        "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "    (\"clf\", LogisticRegression(C=1.0, max_iter=3000, n_jobs=-1,\n",
        "                               multi_class=\"multinomial\", solver=\"lbfgs\"))\n",
        "])\n",
        "\n",
        "# 3) RBF SVM (PCA→256 for speed)\n",
        "models[\"SVM_RBF_PCA256\"] = Pipeline([\n",
        "    (\"pca\", PCA(n_components=256, random_state=42)),\n",
        "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "    (\"clf\", SVC(kernel=\"rbf\", C=5.0, gamma=\"scale\"))\n",
        "])\n",
        "\n",
        "# 4) Linear SGD (hinge) — very fast linear baseline\n",
        "models[\"SGD_Hinge\"] = Pipeline([\n",
        "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "    (\"clf\", SGDClassifier(loss=\"hinge\", alpha=1e-4, max_iter=3000,\n",
        "                          tol=1e-3, n_jobs=-1, random_state=42))\n",
        "])\n",
        "\n",
        "# 5) kNN (PCA→128, k=7)\n",
        "models[\"kNN_PCA128_k7\"] = Pipeline([\n",
        "    (\"pca\", PCA(n_components=128, random_state=42)),\n",
        "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "    (\"clf\", KNeighborsClassifier(n_neighbors=7, n_jobs=-1))\n",
        "])\n",
        "\n",
        "# 6) Random Forest (no scaling needed)\n",
        "models[\"RandomForest400\"] = Pipeline([\n",
        "    (\"clf\", RandomForestClassifier(n_estimators=400, max_depth=None,\n",
        "                                   n_jobs=-1, random_state=42))\n",
        "])\n",
        "\n",
        "# 7) MLP (1 hidden layer)\n",
        "models[\"MLP_512\"] = Pipeline([\n",
        "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "    (\"clf\", MLPClassifier(hidden_layer_sizes=(512,), activation=\"relu\",\n",
        "                          alpha=1e-4, batch_size=256, learning_rate_init=1e-3,\n",
        "                          max_iter=200, random_state=42))\n",
        "])\n",
        "\n",
        "# Evaluate all on (train -> val/test)\n",
        "results = []\n",
        "for name, pipe in models.items():\n",
        "    res = fit_eval(name, pipe, X_tr, y_tr, X_va, y_va, X_te, y_te)\n",
        "    results.append(res)\n",
        "\n",
        "# Rank by validation accuracy\n",
        "results_sorted = sorted(results, key=lambda r: r[\"val_acc\"], reverse=True)\n",
        "print(\"\\n=== Summary (sorted by Val acc) ===\")\n",
        "for r in results_sorted:\n",
        "    print(f\"{r['name']:>18} | Val: {r['val_acc']*100:6.2f}% | Test: {r['test_acc']*100:6.2f}%\")\n",
        "\n",
        "# Refit best on TRAIN+VAL and evaluate on TEST\n",
        "best = results_sorted[0]\n",
        "X_trva = np.concatenate([X_tr, X_va], axis=0)\n",
        "y_trva = np.concatenate([y_tr, y_va], axis=0)\n",
        "best_refit = models[best[\"name\"]]\n",
        "best_refit.fit(X_trva, y_trva)\n",
        "y_test_pred_final = best_refit.predict(X_te)\n",
        "final_test_acc = accuracy_score(y_te, y_test_pred_final)\n",
        "\n",
        "print(f\"\\n=== FINAL (refit on train+val) — {best['name']} ===\")\n",
        "print(f\"Final Test Accuracy: {final_test_acc*100:.2f}%\")\n",
        "print(\"\\nClassification report (Test):\")\n",
        "print(classification_report(y_te, y_test_pred_final, target_names=classes, digits=4))\n",
        "\n",
        "cm = confusion_matrix(y_te, y_test_pred_final)\n",
        "print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
        "print(cm)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPEKLXZ4m23zz638iz9tRcH",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}